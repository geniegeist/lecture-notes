\documentclass[a4paper, 11pt]{article}
\usepackage{marginnote}


\def\nterm {}
\def\nyear {}
\def\nlecturer {}
\def\ncourse {Algebraic Statistics}

\input{header}

\newcommand{\PhantC}{\phantom{\colon}}%
\newcommand{\PhantSQ}{\phantom{\sqrt{\hspace{0.3ex}}}}%
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}}
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

% https://tex.stackexchange.com/questions/63355/wrapping-cmidrule-in-a-macro
\ExplSyntaxOn
\makeatletter
\newcommand{\CMidRule}{\noalign\bgroup\@CMidRule{}}
\NewDocumentCommand{\@CMidRule}{
    m % Material to reinsert before cmidrule.
    O{0.0ex} % #1 = left adjust
    O{0.0ex} % #1 = right adjust
    m  %       #3 = columns to span
}{
    \peek_meaning_remove_ignore_spaces:NTF \CMidRule
      { \@CMidRule { #1 \cmidrule[\cmidrulewidth](l{#2}r{#3}){#4} } }
      { \egroup #1 \cmidrule[\cmidrulewidth](l{#2}r{#3}){#4} }
}
\makeatother
\ExplSyntaxOff


\begin{document}
\maketitle
\tableofcontents

\include{chapters/dimension-theory}





\section{Maximum Likelihood Estimation}

\begin{defi}[Parameter space]
  An open subset \( \Theta \subset \mathbb R^d \) is called the \textbf{parameter space}. Elements \( \theta = (\theta_1,...,\theta_d) \in \Theta \) are called \textbf{parameters}.
\end{defi}

\begin{defi}[Algebraic statistical model]
  An \textbf{algebraic statistical model} is a map \( \mathbf f = (f_1,...,f_m): \mathbb C^d \to \mathbb C^m \) with \(f_i \in \mathbb Q[\theta_1,...,\theta_d] \) such that 
  \begin{itemize}
    \item \( f_1 + ... + f_m - 1 = 0 \in \mathbb Q[\theta_1,...,\theta_d] \) is the zero polynomial, and 
    \item \( \mathbf f(\theta) > 0 \) for all parameters \( \theta \in \Theta \).
  \end{itemize}
  For each parameter \( \theta \in \Theta \) a statistical model \( \mathbf f \) defines a {\textbf{probability distribution}} on the state space \( \left\{ 1,...,m \right\} \), that is, \( f_i(\theta) = p_i \) means that state \( i \in \left\{ 1,...,m \right\} \) occurs with probability \( p_i \in [0,1] \) for parameter \( \theta \).
\end{defi}

Assume we are given the number of occurrences of states \( 1,...,m \) of an experiment by a vector \( \mathbf u = (u_1,...,u_m) \in \mathbb N^m \). Fix a parameter \( \theta \in \Theta \). The probability that the state \( i \in \left\{ 1,...,m \right\} \) appears \( u_i \) times is given by
\begin{align*}
  f_i(\theta)^{u_i}.
\end{align*}
The problem of \textbf{maximum likelihood estimation} is to find the best parameter \( \theta \) that maximizes \(   \prod_{i=1}^m f_i(\theta)^{p_i}\). Maximizing this function is equivalent to maximizing the so called \textbf{log-likelihood function}
\begin{align*}
  \ell_u(\theta) = \sum^m_{i=1}u_i \cdot \log{f_i(\theta)}.
\end{align*}
From calculus, we know that a necessary condition for a local and global maximum \( \hat \theta \) is that the derivative of \( \ell_u \) must vanish at \( \hat \theta \) (note that if \( \Theta \) were not open, then the derivative need not vanish at a global maximum; on the other hand a global maximum need no exist). Thus, we need to find a solution to \( d \)-many equations, called the \textbf{critical equations}
\begin{align*}
  \frac{\partial \ell_u}{\partial \theta_1} &= \sum^m_{i=1} \frac{u_i}{f_i} \frac{\partial f_i}{\partial \theta_1} = 0 \\
  &... \\
  \frac{\partial \ell_u}{\partial \theta_d} &= \sum^m_{i=1} \frac{u_i}{f_i} \frac{\partial f_i}{\partial \theta_d} = 0 \\
\end{align*}

\begin{mdframed}
\begin{center}
  \textbf{{Our goal is to find all solutions \( \theta \in \mathbb C^d \) to the critical equations.}}
\end{center}
\end{mdframed}

Let \( \mathcal H \) be the locus where all the denominators of the rational functions in the critical equations vanish. The set of solutions \( \theta \in \Theta \) outside \( \mathcal H \) is an \emph{algebraic variety} in \( \mathbb C^d \) called the \textbf{likelihood variety}.

\begin{prop}
  For generic data \( u \), the number of solutions to the critical equations is independent of \( u \).
\end{prop}

\begin{proof}
  \begin{align*}
    \frac{\partial}{\partial \theta_i} \log{\frac{f_j}{g_j}} = \frac{g_j}{f_j} \cdot \left( \frac{\partial f_j g_j - \partial g_j f_j}{g_j^2} \right) = \frac{\partial f_j g_j - \partial g_j f_j}{f_j g_j} = \frac{\partial f_j}{f_j} - \frac{\partial g_j}{g_j}
  \end{align*}
\end{proof}

\subsection{Computing the likelihood variety}

The ideal \( (\frac{\partial \ell_u}{\partial \theta_1}, \dots, \frac{\partial \ell_u}{\partial \theta_d}) \) is generated by \emph{rational} functions. Let's find another set of generators that consists of only polynomials. We introduce unknowns \( z = z_1,...,z_m \) where \( z_i \) represents \( f_i^{-1} = \frac{1}{f_i} \). So, we have two polynomial rings \( \mathbb Q[\theta] \) and \( \mathbb Q[\theta, z] \); clearly 
\begin{align*}
  \mathbb Q[\theta] \xhookrightarrow{} \mathbb Q[\theta, z].
\end{align*}
Consider the ideal \( J_u \) generated by \( d + m \) polynomials in \( \mathbb Q [\theta, z] \)
\begin{align*}
  J_u \coloneqq \left(
    \sum^m_{i=1}u_i z_i\frac{\partial f_i}{\partial \theta_1}, 
    \dots,
    \sum^m_{i=1}u_i z_i\frac{\partial f_i}{\partial \theta_d}, 
    z_1 f_1 - 1, \dots , z_m f_m - 1 
  \right).
\end{align*}
A point \( (\theta, z) \in \mathbb C^{d+m} \) lies in the variety \( V(J_u) \) if and only if
\begin{enumerate}
  \item \( \theta \) is a solution to the critical equations,
  \item \( f_i(\theta) \neq 0 \), and
  \item \( z_i = f_i^{-1}(\theta) \).
\end{enumerate}
 Next, we compute the \textbf{elimination ideal} of \( J_u \) in \( \mathbb Q[\theta] \), that is 
\begin{align*}
  I_u \coloneqq J_u \cap \mathbb Q[\theta]
\end{align*}
We call \( I_u \) the \textbf{likelihood ideal} of the model \( \mathbf f \) with respect to the data \( u \). A point \( \theta \in \mathbb C^d \) with \( f_i(\theta) \neq 0 \) lies in \( V(I_u) \) if and only if \( \theta \) is solution to the critical equations. \textbf{Thus, \( V(I_u) \) is the likelihood variety.}

\begin{remark}[Algorithm]
  \(  \)
\begin{enumerate}
  \item Compute the likelihood ideal: \( I_u = J_u \cap \mathbb Q [\theta] \)
  \item Compute \( V(I_u) \) (for example by computing a Gr√∂bner basis).
  \item Compute \( S = V(I_u) \cap \mathbf f^{-1}(\Delta) \), where \( \Delta \) is the \( (m-1) \)-dimensional probability simplex.
  \item For each \( \theta \in S \) check if \( \mathbf f(\theta) \) is a local maxima (for example by examining the Hessian matrix).
\end{enumerate}
\end{remark}


\subsection{Maximum likelihood degree}
An important question for computational statistics is this:
\begin{mdframed}
  \begin{center}
    \textbf{{What happens to the estimate \( \hat \theta \) when we vary \( u \)?}}
  \end{center}
\end{mdframed}

\begin{defi}[Algebraic model]
  We say a model \( \mathbf f \) is \textbf{algebraic} if all the \( f_i \) are polynomials or rational functions.
\end{defi}

\begin{prop}[\( \hat \theta \) is an algebraic function of the data \( u \)]
  The maximum likelihood estimate \( \hat \theta \) is an algebraic function of the data \( u \) if \( \mathbf f \) is algebraic. That is, \( \hat \theta_i \) is a zero of a polynomial of the following form 
  \begin{align*}
    a_r(u) x^r + a_{r - 1}(u)x^{r-1} + ... + a_i(u) x + a_0(u),
  \end{align*}
  where each \( a_i \in \mathbb Q[u] \).
\end{prop}

Without loss of generality, we can assume that the polynomial is an \emph{irreducible element} of \( \mathbb Q[u, x] \). This means that \textbf{the discriminant is a nonzero polynomial in \( \mathbb Q[u] \).} 

\begin{defi}[Generic]
  We say that \( u \in \mathbb R^m \) is \textbf{generic} if no discriminant vanishes at \( u \) for all \( i=1,...,m \). Hence, there exist no multiple roots in any field extension (see Wikipedia, section \emph{Zero discriminant}). The generic vectors are dense in \( \mathbb R^m \).
\end{defi}

\begin{defi}[Maximum likelihood degree]
  The \textbf{maximum likelihood degree} or \textbf{ML degree} of an algebraic statistical model is the \emph{number of solutions to the critical equations} for generic data point \( u \in \mathbb R^m \).
\end{defi}


\end{document}
